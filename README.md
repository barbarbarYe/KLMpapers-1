# Knowledge-enhanced Pretrained Language Model Papers

Contributed by **Chuanzhi Zhuang**  and **[Ningyu Zhang](https://zxlzr.github.io/)**.
 

 
1. **Knowledge Enhanced Contextual Word Representations**. *Peters, Matthew E. Neumann, Mark*. EMNLP 2019.[pdf](https://arxiv.org/pdf/1909.04164.pdf) [code](https://github.com/allenai/kb)
2. **K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters**.ICML2020[pdf](https://arxiv.org/pdf/2002.01808.pdf) 
3. **ERNIE: Enhanced Language Representation with Informative Entities**Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun and Qun Liu. ACL 2019. [pdf](https://www.aclweb.org/anthology/P19-1139) [code & model](https://github.com/thunlp/ERNIE) (ERNIE (Tsinghua) )
4. **ERNIE: Enhanced Representation through Knowledge Integration**.ACL2019[pdf](https://arxiv.org/pdf/1904.09223.pdf) [code](https://github.com/PaddlePaddle/ERNIE/tree/develop/ERNIE) (ERNIE (Baidu) )
5. **K-BERT: Enabling Language Representation with Knowledge Graph**.AAAI2020 [pdf](https://arxiv.org/pdf/1909.07606.pdf),[code](https://github.com/autoliuweijie/K-BERT)
6. **ERNIE 2.0: A Continual Pre-training Framework for Language Understanding**.AAAI2020 [pdf](https://arxiv.org/pdf/1907.12412v1.pdf) [code](https://github.com/PaddlePaddle/ERNIE/blob/develop/README.md)
7. **Semantics-aware BERT for Language Understanding**.AAAI2020  [pdf](https://arxiv.org/abs/1909.02209),[code](https://github.com/cooelf/SemBERT)
8. **Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model**. ICLR2020  [pdf](https://openreview.net/pdf?id=BJlzm64tDH)
9. **KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation**. [pdf](https://arxiv.org/pdf/1911.06136.pdf),[dataset](https://deepgraphlearning.github.io/project/wikidata5m)
10. **Integrating Graph Contextualized Knowledge into Pre-trained Language Models**.[pdf](https://arxiv.org/pdf/1912.00147.pdf)
11. **Informing Unsupervised Pretraining with External Linguistic Knowledge**. [pdf](https://arxiv.org/pdf/1909.02339.pdf)
12. **SpanBERT: Improving Pre-training by Representing
    and Predicting Spans**.[pdf](https://arxiv.org/pdf/1907.10529.pdf),[code&models](https://github.com/facebookresearch/SpanBERT)
 
 
